╔═══════════════════════════════════════════════════════════════════════════╗
║                   OPENALEX DATA PIPELINE SYSTEM OVERVIEW                  ║
╚═══════════════════════════════════════════════════════════════════════════╝

┌───────────────────────────────────────────────────────────────────────────┐
│ ARCHITECTURE FLOW                                                         │
└───────────────────────────────────────────────────────────────────────────┘

    ┌─────────────┐
    │  S3 Bucket  │  s3://openalex (Public, Read-Only)
    │   OpenAlex  │
    └──────┬──────┘
           │
           │ aws s3 sync --no-sign-request
           │ (sync_openalex.sh)
           ▼
    ┌─────────────┐
    │ Local JSONL │  ./openalex_data/data/
    │  .gz Files  │  ├── authors/
    │             │  ├── works/
    │ SOURCE OF   │  ├── institutions/
    │   TRUTH     │  └── ... (11 entity types)
    │ (READ-ONLY) │
    └──────┬──────┘  2,078 files (~1TB)
           │
           │ DuckDB ETL with State Tracking
           │ (process_data.py)
           │ • MD5 hashing for change detection
           │ • SQLite state database
           │ • Incremental processing
           ▼
    ┌─────────────┐
    │  Parquet    │  ./openalex_parquet/
    │   Files     │  ├── authors/
    │             │  ├── works/
    │ (ZSTD       │  └── ...
    │  Compressed)│
    └──────┬──────┘
           │
           │ Wildcard Pattern Views
           │ (init_duckdb.sql)
           │
           ▼
    ┌─────────────┐
    │  Metabase   │  http://localhost:3000
    │  + DuckDB   │  • Interactive dashboards
    │             │  • SQL queries
    │  Analytics  │  • Auto-discovery of new data
    └─────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ FILE INVENTORY                                                            │
└───────────────────────────────────────────────────────────────────────────┘

SETUP & INSTALLATION:
  ✓ install_dependencies.sh    - One-time: Install all system dependencies
  ✓ requirements.txt           - Python package dependencies
  ✓ setup_metabase.sh          - One-time: Setup Metabase + DuckDB plugin
  ✓ validate_pipeline.sh       - Verify setup and data integrity

PIPELINE COMPONENTS:
  ✓ sync_openalex.sh           - Phase 1: S3 → Local sync
  ✓ process_data.py            - Phase 2: JSONL → Parquet ETL
  ✓ run_pipeline.sh            - Phase 4: Master orchestrator
  ✓ init_duckdb.sql            - Phase 3: DuckDB view definitions

CONFIGURATION:
  ✓ docker-compose.yml         - Metabase service definition

DOCUMENTATION:
  ✓ QUICKSTART.md              - Quick reference guide
  ✓ README.md                  - Comprehensive documentation
  ✓ SYSTEM_OVERVIEW.txt        - This file

┌───────────────────────────────────────────────────────────────────────────┐
│ KEY FEATURES                                                              │
└───────────────────────────────────────────────────────────────────────────┘

  ✓ INCREMENTAL PROCESSING
    - Only processes new/modified files (MD5 hash comparison)
    - SQLite database tracks processed files
    - Safe to run daily/weekly

  ✓ FAULT TOLERANCE
    - Failed files logged separately
    - Don't block successful processing
    - Automatic retry on next run

  ✓ DATA INTEGRITY
    - Source data never modified (read-only)
    - State tracking ensures no data loss
    - Parquet validation on read

  ✓ SCHEMA EVOLUTION
    - union_by_name=true handles new columns
    - Wildcard views auto-discover new files
    - No Metabase restart needed

  ✓ PRODUCTION READY
    - Comprehensive logging
    - Error handling
    - Resource monitoring
    - Cron automation

┌───────────────────────────────────────────────────────────────────────────┐
│ QUICK START (3 STEPS)                                                     │
└───────────────────────────────────────────────────────────────────────────┘

  1. Install dependencies
     $ ./install_dependencies.sh

  2. Setup Metabase
     $ ./setup_metabase.sh

  3. Run pipeline
     $ ./run_pipeline.sh

┌───────────────────────────────────────────────────────────────────────────┐
│ DIRECTORY STRUCTURE                                                       │
└───────────────────────────────────────────────────────────────────────────┘

/home/ubuntu/
├── Scripts (Executables)
│   ├── sync_openalex.sh
│   ├── process_data.py
│   ├── run_pipeline.sh
│   ├── setup_metabase.sh
│   ├── install_dependencies.sh
│   └── validate_pipeline.sh
│
├── Configuration
│   ├── docker-compose.yml
│   ├── init_duckdb.sql
│   └── requirements.txt
│
├── Data Directories
│   ├── openalex_data/          ← Source data (read-only)
│   │   └── data/
│   │       ├── authors/
│   │       ├── works/
│   │       └── ... (11 types)
│   │
│   ├── openalex_parquet/       ← Converted Parquet files
│   │   ├── authors/
│   │   ├── works/
│   │   └── ...
│   │
│   └── logs/                   ← All log files
│       ├── sync_*.log
│       ├── pipeline_*.log
│       ├── etl_process.log
│       ├── etl_errors.log
│       └── cron.log
│
├── State & Plugins
│   ├── etl_state.db            ← SQLite tracking database
│   └── metabase_plugins/
│       └── duckdb.metabase-driver.jar
│
└── Documentation
    ├── README.md
    ├── QUICKSTART.md
    └── SYSTEM_OVERVIEW.txt

┌───────────────────────────────────────────────────────────────────────────┐
│ AUTOMATION SETUP                                                          │
└───────────────────────────────────────────────────────────────────────────┘

Add to crontab for automated execution:

  # Edit crontab
  $ crontab -e

  # Weekly execution (Sunday 2 AM)
  0 2 * * 0 cd /home/ubuntu && ./run_pipeline.sh >> ./logs/cron.log 2>&1

  # Bi-weekly execution (1st and 15th at 3 AM)
  0 3 1,15 * * cd /home/ubuntu && ./run_pipeline.sh >> ./logs/cron.log 2>&1

┌───────────────────────────────────────────────────────────────────────────┐
│ MONITORING & VALIDATION                                                   │
└───────────────────────────────────────────────────────────────────────────┘

  Check pipeline status:
    $ ./validate_pipeline.sh

  View logs:
    $ tail -f ./logs/etl_process.log
    $ tail -f ./logs/cron.log

  Query state database:
    $ sqlite3 etl_state.db "SELECT entity_type, COUNT(*) FROM processed_files GROUP BY entity_type;"

  Check Parquet files:
    $ find ./openalex_parquet -name "*.parquet" | wc -l
    $ du -sh ./openalex_parquet

  Test data access:
    $ python3 -c "
    import duckdb
    con = duckdb.connect(':memory:')
    result = con.execute('SELECT COUNT(*) FROM read_parquet(\"./openalex_parquet/works/**/*.parquet\")').fetchone()
    print(f'Total works: {result[0]:,}')
    "

┌───────────────────────────────────────────────────────────────────────────┐
│ ENTITY TYPES                                                              │
└───────────────────────────────────────────────────────────────────────────┘

  1. works         - Publications, articles, papers (LARGEST)
  2. authors       - Author profiles
  3. institutions  - Universities, research organizations
  4. sources       - Journals, conferences
  5. concepts      - Research topics/keywords
  6. topics        - Subject classifications
  7. funders       - Funding organizations
  8. publishers    - Publishing companies
  9. fields        - Academic fields
  10. subfields    - Academic subfields
  11. domains      - High-level academic domains

┌───────────────────────────────────────────────────────────────────────────┐
│ METABASE ACCESS                                                           │
└───────────────────────────────────────────────────────────────────────────┘

  URL: http://localhost:3000

  Initial setup:
    1. Create admin account
    2. Add Database → DuckDB
    3. Name: OpenAlex
    4. Database: :memory: (or /data/openalex.duckdb)
    5. Run init_duckdb.sql in SQL editor

  Example queries:
    - SELECT * FROM works WHERE publication_year = 2024 LIMIT 100;
    - SELECT * FROM authors WHERE cited_by_count > 1000;
    - SELECT COUNT(*) FROM works GROUP BY publication_year;

┌───────────────────────────────────────────────────────────────────────────┐
│ CRITICAL CONSTRAINTS                                                      │
└───────────────────────────────────────────────────────────────────────────┘

  ⚠ NEVER modify ./openalex_data - This is your Source of Truth
  ⚠ Monitor disk space - Keep 100GB+ free
  ⚠ Check failed files regularly - Review etl_state.db
  ⚠ Backup state database - cp etl_state.db etl_state.db.backup

┌───────────────────────────────────────────────────────────────────────────┐
│ TROUBLESHOOTING                                                           │
└───────────────────────────────────────────────────────────────────────────┘

  Problem: Pipeline fails
  Solution: Check ./logs/etl_errors.log and ./logs/etl_process.log

  Problem: Out of disk space
  Solution:
    - find ./logs -name "*.log" -mtime +30 -delete
    - rm -rf ./openalex_data/legacy-data

  Problem: Metabase won't connect
  Solution:
    - docker-compose logs -f metabase
    - docker-compose restart metabase
    - Verify ./metabase_plugins/duckdb.metabase-driver.jar exists

  Problem: State corruption
  Solution:
    - cp etl_state.db etl_state.db.backup
    - sqlite3 etl_state.db "DELETE FROM failed_files;"

┌───────────────────────────────────────────────────────────────────────────┐
│ SUPPORT                                                                   │
└───────────────────────────────────────────────────────────────────────────┘

  Documentation:
    - README.md          - Full documentation
    - QUICKSTART.md      - Quick reference
    - OpenAlex Docs      - https://docs.openalex.org

  Logs:
    - ./logs/            - All pipeline logs
    - etl_state.db       - Processing state

╔═══════════════════════════════════════════════════════════════════════════╗
║                        SYSTEM CREATED: 2025-12-12                         ║
║                          VERSION: 1.0.0                                   ║
╚═══════════════════════════════════════════════════════════════════════════╝
